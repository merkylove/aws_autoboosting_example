{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Amazon SageMaker Model Monitor\n",
    "\n",
    "Amazon SageMaker provides the ability to monitor machine learning models in production and detect deviations in data quality in comparison to a baseline dataset (e.g. training data set). This notebook walks you through enabling data capture and setting up continous monitoring for an existing Endpoint.\n",
    "\n",
    "This Notebook helps with the following:\n",
    "* Update your existing SageMaker Endpoint to enable Model Monitoring\n",
    "* Analyze the training dataset to generate a baseline constraint\n",
    "* Setup a MonitoringSchedule for monitoring deviations from the specified baseline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Enable real-time inference data capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Please provide the Endpoint name in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in the following for enabling data capture\n",
    "endpoint_name = 'titanic-simple-estimator-endpoint2'\n",
    "s3_capture_upload_path = 's3://titanic-dataset-2020-06/outlogs_capture/' #example: s3://bucket-name/path/to/endpoint-data-capture/\n",
    "\n",
    "##### \n",
    "## IMPORTANT\n",
    "##\n",
    "## Please make sure to add the \"s3:PutObject\" permission to the \"role' you provided in the SageMaker Model \n",
    "## behind this Endpoint. Otherwise, Endpoint data capture will not work.\n",
    "## \n",
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'titanic-simple-estimator-endpoint2',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:410029928830:endpoint/titanic-simple-estimator-endpoint2',\n",
       " 'EndpointConfigName': 'titanic-simple-estimator-endpoint2-2020-06-14-23-54-36-008',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3',\n",
       "     'ResolvedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost@sha256:e1523eae041760557744c88335ef19ed82d3cfdc264c5029049b1d542a09b2a6',\n",
       "     'ResolutionTime': datetime.datetime(2020, 6, 14, 23, 54, 38, 845000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'DataCaptureConfig': {'EnableCapture': True,\n",
       "  'CaptureStatus': 'Started',\n",
       "  'CurrentSamplingPercentage': 50,\n",
       "  'DestinationS3Uri': 's3://titanic-dataset-2020-06/outlogs_capture/'},\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2020, 6, 14, 23, 21, 4, 954000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 6, 15, 0, 2, 54, 514000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '5fc1c0ae-b0f1-4fe2-ae19-602d9a15031a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5fc1c0ae-b0f1-4fe2-ae19-602d9a15031a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '946',\n",
       "   'date': 'Mon, 15 Jun 2020 00:03:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker import session\n",
    "import boto3\n",
    "sm_session = session.Session(boto3.Session())\n",
    "\n",
    "# Change parameters as you would like - adjust sampling percentage, \n",
    "#  chose to capture request or response or both.\n",
    "#  Learn more from our documentation\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture = True,\n",
    "                        sampling_percentage=50,\n",
    "                        destination_s3_uri=s3_capture_upload_path,\n",
    "                        kms_key_id=None,\n",
    "                        capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "                        csv_content_types=[\"text/csv\"],\n",
    "                        json_content_types=[\"application/json\"])\n",
    "\n",
    "# Now it is time to apply the new configuration and wait for it to be applied\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name)\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "sm_session.wait_for_endpoint(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you proceed:\n",
    "Currently SageMaker supports monitoring Endpoints out of the box only for **tabular (csv, flat-json)** datasets. If your Endpoint uses some other datasets, these following steps will NOT work for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Monitor - Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, SageMaker allows you to monitor and evaluate the data observed by the Endpoints. For this :\n",
    "1. We need to create a baseline with which we compare the realtime traffic against. \n",
    "1. Once a baseline is ready, we can setup a schedule to continously evaluate/compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset's data schema and the inference dataset schema should exactly match (i.e. number and order of the features).\n",
    "\n",
    "Using our training dataset, we'll ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data_uri = 'FILL-ME-IN' ##'s3://bucketname/path/to/baseline/data' - Where your training data is\n",
    "baseline_results_uri = 'FILL-ME-IN' ##'s3://bucketname/path/to/baseline/data' - Where the results are to be stored in\n",
    "\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baselining job with the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training data ready in S3, let's kick off a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` kicks off a `ProcessingJob` using a SageMaker provided Model Monitor container to generate the constraints. Please edit the configurations to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri+'/training-dataset-with-header.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to enable monitoring, you could chose to edit the constraint file as required to fine tune the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Enable continous monitoring\n",
    "\n",
    "We have collected the data above, here we proceed to analyze and monitor the data with MonitoringSchedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a model monitoring schedule for the Endpoint created earlier with the baseline resources (constraints and statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'FILL-IN-HERE'\n",
    "s3_report_path = 'FILL-IN-HERE'\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All set\n",
    "Now that your monitoring schedule has been created. Please return to the Amazon SageMaker Studio to list the executions for this Schedule and observe the results going forward."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
